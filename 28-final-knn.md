# KNN

## האם KNN פותר בעיית קלסיפיקציה, רגרסיה או גם וגם?

**תשובה**
KNN יכול לשמש גם לסיווג וגם לרגרסיה. בדרך כלל משתמשים בו לסיווג, אבל אפשר בקלות להתאים אותו גם לחיזוי ערכים רציפים.

---

## מה המשמעות של השם KNN?

**תשובה**  
KNN = K Nearest Neighbors, כלומר "K השכנים הקרובים ביותר". הרעיון: בשביל נקודה חדשה מסתכלים על K הדוגמאות הכי קרובות ומחליטים לפי מה שקורה אצלן.

---

## מה הרעיון המרכזי של KNN?

**תשובה**
  
נקודות דומות נוטות לקבל תוצאה דומה. לכן, כדי לנבא עבור נקודה חדשה, מחפשים את השכנים הקרובים אליה במרחב הפיצ'רים ומבוססים עליהם כדי לקבוע את התוצאה.

---

## מה התפקיד של K באלגוריתם?

**תשובה**
  
K קובע כמה שכנים נכניס לחישוב. למשל, כש־K=3 משתמשים בשלוש הנקודות הקרובות ביותר כדי להחליט מה תהיה התחזית.

---

## האם K הוא Hyperparameter? הסבר

**תשובה**
  
כן. K הוא Hyperparameter כי מחליטים עליו מראש, ולא לומדים אותו מהנתונים. שינוי הערך שלו משנה את אופי המודל ואת רמת ההתאמה לנתונים.

---

## איך ערך קטן של K משפיע על המודל?

**תשובה**
  
ערך קטן של K (למשל 1) עושה את המודל מאוד רגיש לנתונים:
- מתאים מאוד לנתוני האימון  
- רגיש מאוד לרעש ולנקודות חריגות  
- עלול לגרום ל־overfitting

---

## איך ערך גדול של K משפיע על המודל?

**תשובה**
  
ערך גדול של K:
- מחליק את ההחלטות ומפחית השפעה של רעש  
- הופך את המודל ליציב יותר  
- אם K גדול מדי, המודל נהיה גס מדי ועלול להגיע ל־underfitting

---

## מה זה גרף Elbow ואיך בוחרים Sweet Spot?

**תשובה**
  
גרף Elbow מציג איך הביצועים משתנים כשמגדילים פרמטר (למשל K).

- ציר X: ערכי הפרמטר (למשל K).  
- ציר Y: מדד ביצועים או שגיאה (למשל Accuracy או MSE).

בדרך כלל רואים שיפור מהיר בהתחלה ואז האטה.  
ה־Sweet Spot הוא בערך שבו השיפור כמעט נעצר – נקודת ה"מרפק".

דוגמה נוספת:  
ב־K-Means משתמשים באותו רעיון כדי לבחור את מספר הקלאסטרים K:
- X: מספר הקבוצות  
- Y: סכום ריבועי המרחקים בתוך הקבוצות  
הנקודה שבה הירידה בשגיאה מתחילה להאט נחשבת מספר קבוצות טוב.

---

## מה ההבדל בין KNN לקלסיפיקציה לבין KNN לרגרסיה?

**תשובה**
  
- בקלסיפיקציה: בוחרים את המחלקה שמופיעה הכי הרבה בין K השכנים.  
- ברגרסיה: מחשבים ממוצע (או ממוצע משוקלל) של ערכי ה־Y של K השכנים.

---

## איך מתקבלת תחזית ב־KNN לקלסיפיקציה?

**תשובה**
  
1. מחשבים מרחק בין הנקודה החדשה לכל נקודות האימון.  
2. מוצאים את K הנקודות הכי קרובות.  
3. בודקים איזו מחלקה מופיעה הכי הרבה בין השכנים.  
4. משייכים את הנקודה למחלקה הזו.

---

## איך מתקבלת תחזית ב־KNN לרגרסיה?

**תשובה**
  
1. מחשבים את המרחק לכל נקודות האימון.  
2. בוחרים את K השכנים הקרובים ביותר.  
3. לוקחים את ערכי ה־Y שלהם.  
4. מחשבים ממוצע (או ממוצע משוקלל) – זה הערך החזוי.

---

## איזה מדד מרחק נפוץ ב־KNN?

**תשובה**
  
בדרך כלל משתמשים במרחק אוקלידי:  
שורש סכום ריבועי ההפרשים בין הפיצ'רים של שתי נקודות.

---

## איך מספר הפיצ'רים משפיע על ביצועי KNN?

**תשובה**
  
- כשיש יותר מדי פיצ'רים:  
  - המרחב נהיה "גבוה־מימד"  
  - כולם בערך רחוקים מכולם  
  - קשה למצוא שכנים "באמת" קרובים → ביצועים יורדים

- כשמסירים פיצ'רים:  
  - החישוב מהיר ופשוט יותר  
  - אם מסירים פיצ'רים חשובים מדי – המודל מאבד מידע ומנבא פחות טוב

---

## האם יש שלב אימון מובהק ב־KNN?

**תשובה**
  
לא ממש.  
ב־fit האלגוריתם פשוט שומר את הנתונים.  
העבודה האמיתית נעשית בזמן החיזוי, כשמחשבים מרחקים לנקודה החדשה ומוצאים את K השכנים. לכן KNN נקרא לעיתים "לומד עצל" (Lazy Learner).

---

## איך Python פותרת KNN – נוסחה סגורה או חישוב בזמן החיזוי?

**תשובה**
  
במימושים כמו `scikit-learn`:
- בשלב fit שומרים את הנתונים (לפעמים בתוך מבנה כמו KD-Tree).  
- בשלב predict מחשבים בפועל את המרחקים ומוצאים שכנים.  

אין נוסחה סגורה – זה חישוב ישיר בזמן החיזוי.

---

## מה היתרונות המרכזיים של KNN?

**תשובה**
  
- פשוט וקל להסבר  
- מתאים לקלסיפיקציה ולרגרסיה  
- לא מניח צורה לינארית כלשהי של הנתונים  
- אפשר לעדכן בקלות בעזרת הוספת דוגמאות חדשות בלי לאמן מודל מחדש

---

## מה החסרונות המרכזיים של KNN?

**תשובה**
  
- חישוב יקר בזמן חיזוי (צריך לחשב מרחקים להרבה נקודות)  
- רגיש לבחירת K  
- סובל מקללת המימדיות כשיש הרבה פיצ'רים  
- דורש נרמול פיצ'רים, אחרת פיצ'ר אחד יכול לשלוט במרחק  
- אין מודל פרמטרי ברור שנותן תובנות (כמו משקולות)

---

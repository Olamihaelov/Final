# KNN

האם האלגוריתם KNN פותר בעיית קלסיפיקציה, רגרסיה או גם וגם?

**תשובה**  
KNN יכול לשמש גם לסיווג וגם לרגרסיה. בדרך כלל משתמשים בו לסיווג, אבל אפשר בקלות להתאים אותו גם לחיזוי ערכים רציפים.

---

מה המשמעות של השם KNN?

**תשובה**  
KNN = K Nearest Neighbors, כלומר "K השכנים הקרובים ביותר". הרעיון: בשביל נקודה חדשה מסתכלים על K הדוגמאות הכי קרובות ומחליטים לפי התוצאות שלהן.

---

מהו הרעיון המרכזי שעליו מבוסס האלגוריתם KNN?

**תשובה**  
נקודות דומות נוטות לקבל תוצאה דומה. לכן, כדי לנבא עבור נקודה חדשה, מחפשים את השכנים הקרובים אליה במרחב הפיצ'רים ומשתמשים בהם כדי לקבוע את התחזית.

---

מהו התפקיד של הפרמטר K באלגוריתם?

**תשובה**  
K קובע כמה שכנים נכניס לחישוב. למשל, כש־K=3 משתמשים בשלוש הנקודות הקרובות ביותר כדי להחליט מה תהיה התחזית.

---

האם K נחשב Hyperparameter? הסבר/י

**תשובה**  
כן. K הוא Hyperparameter כי מחליטים עליו מראש, ולא לומדים אותו מהנתונים. שינוי הערך שלו משנה את אופי המודל ואת רמת ההתאמה שלו.

---

כיצד בחירת ערך קטן של K משפיעה על המודל?

**תשובה**  
ערך קטן של K (למשל 1) עושה את המודל מאוד רגיש לנתונים:  
- מתאים מאוד לנתוני האימון  
- רגיש מאוד לרעש ולנקודות חריגות  
- עלול לגרום ל־overfitting

---

כיצד בחירת ערך גדול של K משפיעה על המודל?

**תשובה**  
ערך גדול של K:  
- מחליק את ההחלטות ומפחית השפעה של רעש  
- הופך את המודל ליציב יותר  
- אם K גדול מדי, המודל נהיה גס מדי ועלול להגיע ל־underfitting

---

בכדי למדוד את ה- K האידיאלי נשתמש בגרף ה ELBOW  
הסבר מה זה גרף ELBOW? מה בד"כ יש בציר ה- X וציר ה- Y?  
כיצד נבחר את ה- Sweet Spot?  
תן דוגמא לעוד מודל ששם נשתמש בגרף זה והסבר כיצד?  

**תשובה**  
גרף Elbow (גרף "מרפק") עוזר למצוא את K הכי טוב ב-KNN.  
בציר X שמים את ערכי K השונים (1, 3, 5 וכו'), ובציר Y שמים את השגיאה של המודל (כמה הוא טועה).  
כשמגדילים את K השגיאה יורדת בהתחלה מהר, ואז כמעט לא יורדת יותר – הנקודה שבה הגרף "מתכופף" כמו מרפק היא ה-K המומלץ. 
דוגמה נוספת: ב-K-Means בוחרים מספר אשכולות באותה נקודת מרפק, שם השיפור כבר לא גדול.  

---

מה ההבדל בין KNN לקלסיפיקציה לבין KNN לרגרסיה?

**תשובה**  
- בקלסיפיקציה: בוחרים את המחלקה שמופיעה הכי הרבה בין K השכנים.  
- ברגרסיה: מחשבים ממוצע (או ממוצע משוקלל) של ערכי ה־Y של K השכנים.

---

כיצד מתקבלת התחזית ב־KNN לקלסיפיקציה?

**תשובה**  
1. מחשבים מרחק בין הנקודה החדשה לכל נקודות האימון.  
2. מוצאים את K הנקודות הכי קרובות.  
3. בודקים איזו מחלקה מופיעה הכי הרבה בין השכנים.  
4. משייכים את הנקודה למחלקה הזו.

---

כיצד מתקבלת התחזית ב־KNN לרגרסיה?

**תשובה**  
1. מחשבים את המרחק לכל נקודות האימון.  
2. בוחרים את K השכנים הקרובים ביותר.  
3. לוקחים את ערכי ה־Y שלהם.  
4. מחשבים ממוצע (או ממוצע משוקלל) – זה הערך החזוי.

---

איזו מדידת מרחק נפוצה משמשת ב־KNN?

**תשובה**  
בדרך כלל משתמשים במרחק אוקלידי:  
שורש סכום ריבועי ההפרשים בין הפיצ'רים של שתי נקודות.

---

כיצד מספר הפיצ'רים משפיע על ביצועי KNN?

**תשובה**  
* כשיש יותר מדי פיצ'רים:  
  - המרחב נהיה "גבוה־מימד"  
  - כולם בערך רחוקים מכולם  
  - קשה למצוא שכנים "באמת" קרובים - ביצועים יורדים  

- כשמסירים פיצ'רים:  
  - החישוב מהיר ופשוט יותר  
  - אם מסירים פיצ'רים חשובים מדי – המודל מאבד מידע ומנבא פחות טוב

---

האם קיים שלב אימון (Training) מובהק באלגוריתם KNN?

**תשובה**  
לא ממש.  
ב־fit האלגוריתם פשוט שומר את הנתונים.  
העבודה האמיתית נעשית בזמן החיזוי, כשמחשבים מרחקים לנקודה החדשה ומוצאים את K השכנים.

---

כיצד Python פותר את חישוב KNN – האם באמצעות נוסחה סגורה או באמצעות חישוב ישיר בזמן החיזוי?

**תשובה**  
- בשלב fit שומרים את הנתונים.  
- בשלב predict מחשבים בפועל את המרחקים ומוצאים שכנים.  

אין נוסחה סגורה – זה חישוב ישיר בזמן החיזוי.

---

מהם היתרונות המרכזיים של KNN?

**תשובה**  
- פשוט וקל להסבר  
- מתאים לקלסיפיקציה ולרגרסיה  
- לא מניח צורה לינארית כלשהי של הנתונים  
- אפשר לעדכן בקלות בעזרת הוספת דוגמאות חדשות בלי לאמן מודל מחדש

---

מהם החסרונות המרכזיים של KNN?

**תשובה**  
- חישוב יקר בזמן חיזוי (צריך לחשב מרחקים להרבה נקודות)  
- רגיש לבחירת K  
- סובל מקללת המימדיות כשיש הרבה פיצ'רים  
- דורש נרמול פיצ'רים, אחרת פיצ'ר אחד יכול לשלוט במרחק  
- אין מודל פרמטרי ברור שנותן תובנות (כמו משקולות)

---
